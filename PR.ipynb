{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1AJ4b9ClLKSVJB5qe6r0Vo5HgDCQHhjwE",
      "authorship_tag": "ABX9TyOwsy0lqKlg4h2hNKtJFoUU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gvvishal/-Intelligent-Clothing-Stock-Management-System-using-AI-and-SQL/blob/main/PR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  STEP 1: Mount Google Drive\n"
      ],
      "metadata": {
        "id": "E1C0mEnrHFOq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "v2ZzykkHHHs0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set dataset path\n",
        "dataset_path = \"/content/drive/MyDrive/Binary_Classification\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pukYgVT0Bzxa",
        "outputId": "0640c2de-7992-49dc-a3bb-0ff8302e4366"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  STEP 2: Check Dataset Structure"
      ],
      "metadata": {
        "id": "S0iCtpW9HRUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "for split in [\"Train\", \"Test\"]:\n",
        "    for cls in [\"Tumor\", \"No_Tumor\"]:\n",
        "        folder = os.path.join(dataset_path, split, cls)\n",
        "        print(f\"{split}/{cls}: {len(os.listdir(folder))} images\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJKoQZF5DJI3",
        "outputId": "a767ca1b-5845-4e61-877d-7ce51ac04d73"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train/Tumor: 3904 images\n",
            "Train/No_Tumor: 1058 images\n",
            "Test/Tumor: 755 images\n",
            "Test/No_Tumor: 139 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  STEP 3: Remove Duplicate Images"
      ],
      "metadata": {
        "id": "EQKckRMGHWKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import hashlib\n",
        "from collections import defaultdict\n",
        "from PIL import Image\n",
        "\n",
        "def hash_image(image_path):\n",
        "    \"\"\"Return a hash for an image (so duplicates can be detected).\"\"\"\n",
        "    try:\n",
        "        with Image.open(image_path) as img:\n",
        "            img = img.convert(\"RGB\").resize((256, 256))  # normalize size\n",
        "            return hashlib.md5(img.tobytes()).hexdigest()\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {image_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Step 3.1: Collect hashes\n",
        "hash_map = defaultdict(list)\n",
        "\n",
        "for split in [\"Train\", \"Test\"]:\n",
        "    for cls in [\"Tumor\", \"No_Tumor\"]:\n",
        "        folder = os.path.join(dataset_path, split, cls)\n",
        "        for fname in os.listdir(folder):\n",
        "            fpath = os.path.join(folder, fname)\n",
        "            img_hash = hash_image(fpath)\n",
        "            if img_hash:\n",
        "                hash_map[img_hash].append(fpath)\n",
        "\n",
        "# Step 3.2: Decide which duplicates to delete\n",
        "to_delete = []\n",
        "\n",
        "for img_hash, files in hash_map.items():\n",
        "    if len(files) > 1:\n",
        "        train_files = [f for f in files if \"/Train/\" in f]\n",
        "        test_files  = [f for f in files if \"/Test/\" in f]\n",
        "\n",
        "        # Rule 1: Keep 1 in Train, delete rest\n",
        "        if len(train_files) > 1:\n",
        "            to_delete.extend(train_files[1:])\n",
        "\n",
        "        # Rule 2: If same image appears in Train and Test → delete Test copies\n",
        "        if train_files and test_files:\n",
        "            to_delete.extend(test_files)\n",
        "\n",
        "        # Rule 3: If only in Test, keep 1, delete rest\n",
        "        if not train_files and len(test_files) > 1:\n",
        "            to_delete.extend(test_files[1:])\n",
        "\n",
        "# Step 3.3: Delete duplicates\n",
        "for f in to_delete:\n",
        "    try:\n",
        "        os.remove(f)\n",
        "        print(f\"Deleted duplicate: {f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error deleting {f}: {e}\")\n",
        "\n",
        "print(f\"\\n✅ Done! Removed {len(to_delete)} duplicates.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdTIUfBVGVMl",
        "outputId": "9fcd143f-d895-43b6-e8d6-2360062d160f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Done! Removed 0 duplicates.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "for split in [\"Train\", \"Test\"]:\n",
        "    for cls in [\"Tumor\", \"No_Tumor\"]:\n",
        "        folder = os.path.join(dataset_path, split, cls)\n",
        "        print(f\"{split}/{cls}: {len(os.listdir(folder))} images\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4HaITdCJY37",
        "outputId": "68128f36-d688-4922-a68e-afaca7ec85aa"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train/Tumor: 3904 images\n",
            "Train/No_Tumor: 1058 images\n",
            "Test/Tumor: 755 images\n",
            "Test/No_Tumor: 139 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  STEP 4: Image Generators (Data Loading)\n"
      ],
      "metadata": {
        "id": "Kz5YXsdHICTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_dir = os.path.join(dataset_path, \"Train\")\n",
        "test_dir  = os.path.join(dataset_path, \"Test\")\n",
        "\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Train with augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "# Validation (no augmentation, only rescale)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "\n",
        "# Test (no augmentation)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Generators\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode=\"binary\",\n",
        "    subset=\"training\"\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode=\"binary\",\n",
        "    subset=\"validation\"\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode=\"binary\",\n",
        "    shuffle=False\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSbY1sMjIBy9",
        "outputId": "af525fe0-dcff-40ea-ee75-1bbb79526668"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3971 images belonging to 2 classes.\n",
            "Found 991 images belonging to 2 classes.\n",
            "Found 894 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "H1e_zxDcIBM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for split in [\"Train\", \"Test\"]:\n",
        "    for cls in [\"Tumor\", \"No_Tumor\"]:\n",
        "        folder = os.path.join(dataset_path, split, cls)\n",
        "        print(f\"{split}/{cls}: {len(os.listdir(folder))} images\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pC0GZEYHPR0",
        "outputId": "5f06316d-0ea8-4ca3-b96a-127c83ffd729"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train/Tumor: 3904 images\n",
            "Train/No_Tumor: 1058 images\n",
            "Test/Tumor: 755 images\n",
            "Test/No_Tumor: 139 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#STEP 5: Handle Class Imbalance"
      ],
      "metadata": {
        "id": "snDS87ssISbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "# Count training images\n",
        "tumor_count = len(os.listdir(os.path.join(train_dir, \"Tumor\")))\n",
        "no_tumor_count = len(os.listdir(os.path.join(train_dir, \"No_Tumor\")))\n",
        "\n",
        "print(f\"Train/Tumor: {tumor_count}, Train/No_Tumor: {no_tumor_count}\")\n",
        "\n",
        "# Assign class weights (more weight to minority class)\n",
        "classes = np.array([0]*no_tumor_count + [1]*tumor_count)  # 0=No_Tumor, 1=Tumor\n",
        "weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(classes), y=classes)\n",
        "class_weight = dict(zip(np.unique(classes), weights))\n",
        "\n",
        "print(\"Class Weights:\", class_weight)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ot7NXoKZCJQq",
        "outputId": "0482ac2b-2ad5-4b92-da5b-1f0bdaaacfd8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train/Tumor: 3904, Train/No_Tumor: 1058\n",
            "Class Weights: {np.int64(0): np.float64(2.3449905482041586), np.int64(1): np.float64(0.6355020491803278)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import Sequence\n",
        "import os\n",
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "# =========================\n",
        "# 1. Parameters\n",
        "# =========================\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "train_dir = os.path.join(dataset_path, \"Train\")\n",
        "classes = [\"No_Tumor\", \"Tumor\"]  # class order\n",
        "\n",
        "# =========================\n",
        "# 2. Custom Generator with Oversampling\n",
        "# =========================\n",
        "class BalancedDataGenerator(Sequence):\n",
        "    def __init__(self, directory, classes, batch_size, img_size, augmentor=None):\n",
        "        self.directory = directory\n",
        "        self.classes = classes\n",
        "        self.batch_size = batch_size\n",
        "        self.img_size = img_size\n",
        "        self.augmentor = augmentor\n",
        "\n",
        "        # Collect file paths per class\n",
        "        self.file_paths = []\n",
        "        self.labels = []\n",
        "        for idx, cls in enumerate(classes):\n",
        "            cls_folder = os.path.join(directory, cls)\n",
        "            files = [os.path.join(cls_folder, f) for f in os.listdir(cls_folder)]\n",
        "            self.file_paths.extend(files)\n",
        "            self.labels.extend([idx] * len(files))\n",
        "\n",
        "        # Separate minority and majority class indices\n",
        "        counts = [self.labels.count(i) for i in range(len(classes))]\n",
        "        self.max_count = max(counts)\n",
        "        self.indices_per_class = {i: [j for j, lbl in enumerate(self.labels) if lbl == i] for i in range(len(classes))}\n",
        "\n",
        "    def __len__(self):\n",
        "        # Total batches per epoch\n",
        "        return int(np.ceil(self.max_count * len(self.classes) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_paths = []\n",
        "        batch_labels = []\n",
        "\n",
        "        # Oversample minority class\n",
        "        for cls_idx in range(len(self.classes)):\n",
        "            cls_indices = self.indices_per_class[cls_idx]\n",
        "            sampled = np.random.choice(cls_indices, self.max_count, replace=True)\n",
        "            batch_paths.extend([self.file_paths[i] for i in sampled])\n",
        "            batch_labels.extend([cls_idx] * len(sampled))\n",
        "\n",
        "        # Shuffle batch\n",
        "        combined = list(zip(batch_paths, batch_labels))\n",
        "        random.shuffle(combined)\n",
        "        batch_paths, batch_labels = zip(*combined)\n",
        "\n",
        "        # Take only batch_size images\n",
        "        batch_paths = batch_paths[:self.batch_size]\n",
        "        batch_labels = batch_labels[:self.batch_size]\n",
        "\n",
        "        # Load images and apply augmentation if any\n",
        "        batch_x = []\n",
        "        for p in batch_paths:\n",
        "            img = Image.open(p).convert(\"RGB\").resize(self.img_size)\n",
        "            img_array = np.array(img) / 255.0\n",
        "            if self.augmentor:\n",
        "                img_array = self.augmentor.random_transform(img_array)\n",
        "            batch_x.append(img_array)\n",
        "\n",
        "        return np.array(batch_x), np.array(batch_labels)\n",
        "\n",
        "# =========================\n",
        "# 3. Define Augmentor\n",
        "# =========================\n",
        "train_augmentor = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode=\"nearest\"\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# 4. Create Balanced Generator\n",
        "# =========================\n",
        "balanced_train_generator = BalancedDataGenerator(\n",
        "    directory=train_dir,\n",
        "    classes=classes,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    img_size=IMG_SIZE,\n",
        "    augmentor=train_augmentor\n",
        ")\n"
      ],
      "metadata": {
        "id": "F9TwkshsK0iw"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "classes = [\"No_Tumor\", \"Tumor\"]\n",
        "total_counts = np.zeros(len(classes), dtype=int)\n",
        "\n",
        "for i in range(len(balanced_train_generator)):\n",
        "    _, y_batch = balanced_train_generator[i]\n",
        "    for lbl in y_batch:\n",
        "        total_counts[lbl] += 1\n",
        "\n",
        "for idx, cls in enumerate(classes):\n",
        "    print(f\"{cls}: {total_counts[idx]} effective samples in one epoch\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTYROU4hLPrQ",
        "outputId": "26a6a3e2-5ef8-4ccb-ffd6-3057b24c31d5"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No_Tumor: 3879 effective samples in one epoch\n",
            "Tumor: 3929 effective samples in one epoch\n"
          ]
        }
      ]
    }
  ]
}